{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OpOuE81UdaAn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Optimizer:\n",
        "    \"\"\"Base class for all optimizers\"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        \"\"\"\n",
        "        Update rule to be implemented by specific optimizers\n",
        "        params: dictionary of parameters (W1, b1, W2, b2, etc.)\n",
        "        grads: dictionary of gradients (dW1, db1, dW2, db2, etc.)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Optimizer subclasses must implement update method\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent\n",
        "\n",
        "    Formula:\n",
        "    θ = θ - η * ∇J(θ)\n",
        "\n",
        "    Where:\n",
        "    θ: parameters (weights, biases)\n",
        "    η: learning rate\n",
        "    ∇J(θ): gradient of cost function with respect to parameters\n",
        "    \"\"\"\n",
        "    def update(self, params, grads):\n",
        "        for key in params:\n",
        "            params[key] -= self.learning_rate * grads[\"d\" + key]\n",
        "        return params\n"
      ],
      "metadata": {
        "id": "oYoCQFQxfCa5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDMomentum(Optimizer):\n",
        "    \"\"\"\n",
        "    SGD with Momentum\n",
        "\n",
        "    Formula:\n",
        "    v = γ * v + η * ∇J(θ)\n",
        "    θ = θ - v\n",
        "\n",
        "    Where:\n",
        "    v: velocity (initialized as zeros)\n",
        "    γ: momentum coefficient (typically 0.9)\n",
        "    η: learning rate\n",
        "    ∇J(θ): gradient of cost function\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        super().__init__(learning_rate)\n",
        "        self.momentum = momentum\n",
        "        self.velocity = {}\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if not self.velocity:\n",
        "            # Initialize velocity if it's the first iteration\n",
        "            for key in params:\n",
        "                self.velocity[key] = np.zeros_like(params[key])\n",
        "\n",
        "        for key in params:\n",
        "            # Update velocity\n",
        "            self.velocity[key] = self.momentum * self.velocity[key] + self.learning_rate * grads[\"d\" + key]\n",
        "            # Update parameters\n",
        "            params[key] -= self.velocity[key]\n",
        "\n",
        "        return params\n"
      ],
      "metadata": {
        "id": "j2JTDO7yfId9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSprop(Optimizer):\n",
        "    \"\"\"\n",
        "    Root Mean Square Propagation\n",
        "\n",
        "    Formula:\n",
        "    s = β * s + (1 - β) * (∇J(θ))^2\n",
        "    θ = θ - η * ∇J(θ) / (√s + ε)\n",
        "\n",
        "    Where:\n",
        "    s: squared gradients moving average (initialized as zeros)\n",
        "    β: decay rate (typically 0.9)\n",
        "    η: learning rate\n",
        "    ε: small constant to avoid division by zero\n",
        "    ∇J(θ): gradient of cost function\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, beta=0.9, epsilon=1e-8):\n",
        "        super().__init__(learning_rate)\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.squared_gradients = {}\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if not self.squared_gradients:\n",
        "            # Initialize squared gradients if it's the first iteration\n",
        "            for key in params:\n",
        "                self.squared_gradients[key] = np.zeros_like(params[key])\n",
        "\n",
        "        for key in params:\n",
        "            # Update squared gradients moving average\n",
        "            self.squared_gradients[key] = self.beta * self.squared_gradients[key] + \\\n",
        "                                         (1 - self.beta) * np.square(grads[\"d\" + key])\n",
        "            # Update parameters\n",
        "            params[key] -= self.learning_rate * grads[\"d\" + key] / \\\n",
        "                          (np.sqrt(self.squared_gradients[key]) + self.epsilon)\n",
        "\n",
        "        return params"
      ],
      "metadata": {
        "id": "8VsyQouvfJgb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam(Optimizer):\n",
        "    \"\"\"\n",
        "    Adaptive Moment Estimation\n",
        "\n",
        "    Formula:\n",
        "    m = β1 * m + (1 - β1) * ∇J(θ)               # First moment estimate\n",
        "    v = β2 * v + (1 - β2) * (∇J(θ))^2           # Second moment estimate\n",
        "    m̂ = m / (1 - β1^t)                          # Bias-corrected first moment\n",
        "    v̂ = v / (1 - β2^t)                          # Bias-corrected second moment\n",
        "    θ = θ - η * m̂ / (√v̂ + ε)                    # Parameter update\n",
        "\n",
        "    Where:\n",
        "    m: first moment vector (mean of gradients, initialized as zeros)\n",
        "    v: second moment vector (uncentered variance, initialized as zeros)\n",
        "    β1, β2: decay rates for moment estimates (typically 0.9 and 0.999)\n",
        "    t: time step\n",
        "    η: learning rate\n",
        "    ε: small constant to avoid division by zero\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        super().__init__(learning_rate)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}  # First moment\n",
        "        self.v = {}  # Second moment\n",
        "        self.t = 0   # Time step\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if not self.m:\n",
        "            # Initialize moments if it's the first iteration\n",
        "            for key in params:\n",
        "                self.m[key] = np.zeros_like(params[key])\n",
        "                self.v[key] = np.zeros_like(params[key])\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        for key in params:\n",
        "            # Update biased first moment estimate\n",
        "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[\"d\" + key]\n",
        "            # Update biased second moment estimate\n",
        "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * np.square(grads[\"d\" + key])\n",
        "\n",
        "            # Compute bias-corrected first moment estimate\n",
        "            m_corrected = self.m[key] / (1 - self.beta1 ** self.t)\n",
        "            # Compute bias-corrected second moment estimate\n",
        "            v_corrected = self.v[key] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # Update parameters\n",
        "            params[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
        "\n",
        "        return params"
      ],
      "metadata": {
        "id": "w8hTJBMDfhcQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad(Optimizer):\n",
        "    \"\"\"\n",
        "    Adaptive Gradient Algorithm\n",
        "\n",
        "    Formula:\n",
        "    s = s + (∇J(θ))^2\n",
        "    θ = θ - η * ∇J(θ) / (√s + ε)\n",
        "\n",
        "    Where:\n",
        "    s: sum of squared gradients (initialized as zeros)\n",
        "    η: learning rate\n",
        "    ε: small constant to avoid division by zero\n",
        "    ∇J(θ): gradient of cost function\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
        "        super().__init__(learning_rate)\n",
        "        self.epsilon = epsilon\n",
        "        self.squared_gradients_sum = {}\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if not self.squared_gradients_sum:\n",
        "            # Initialize sum of squared gradients if it's the first iteration\n",
        "            for key in params:\n",
        "                self.squared_gradients_sum[key] = np.zeros_like(params[key])\n",
        "\n",
        "        for key in params:\n",
        "            # Accumulate squared gradients\n",
        "            self.squared_gradients_sum[key] += np.square(grads[\"d\" + key])\n",
        "            # Update parameters\n",
        "            params[key] -= self.learning_rate * grads[\"d\" + key] / \\\n",
        "                          (np.sqrt(self.squared_gradients_sum[key]) + self.epsilon)\n",
        "\n",
        "        return params\n"
      ],
      "metadata": {
        "id": "pTtCOoOOfsbK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adadelta(Optimizer):\n",
        "    \"\"\"\n",
        "    Adadelta Algorithm\n",
        "\n",
        "    Formula:\n",
        "    E[g²]_t = ρ * E[g²]_{t-1} + (1 - ρ) * (∇J(θ))^2\n",
        "    RMS[g]_t = √(E[g²]_t + ε)\n",
        "    Δθ_t = - η * ∇J(θ) / RMS[g]_t\n",
        "    E[Δθ²]_t = ρ * E[Δθ²]_{t-1} + (1 - ρ) * Δθ_t^2\n",
        "    RMS[Δθ]_t = √(E[Δθ²]_t + ε)\n",
        "    θ_t = θ_{t-1} + RMS[Δθ]_{t-1} / RMS[g]_t * ∇J(θ)\n",
        "\n",
        "    Where:\n",
        "    E[g²]: Running average of squared gradients\n",
        "    E[Δθ²]: Running average of squared parameter updates\n",
        "    ρ: Decay constant (typically 0.95)\n",
        "    ε: Small constant for numerical stability\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=1.0, rho=0.95, epsilon=1e-6):\n",
        "        super().__init__(learning_rate)\n",
        "        self.rho = rho\n",
        "        self.epsilon = epsilon\n",
        "        self.avg_squared_grad = {}    # E[g²]\n",
        "        self.avg_squared_delta = {}   # E[Δθ²]\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if not self.avg_squared_grad:\n",
        "            # Initialize if it's the first iteration\n",
        "            for key in params:\n",
        "                self.avg_squared_grad[key] = np.zeros_like(params[key])\n",
        "                self.avg_squared_delta[key] = np.zeros_like(params[key])\n",
        "\n",
        "        for key in params:\n",
        "            # Update running average of squared gradients\n",
        "            self.avg_squared_grad[key] = self.rho * self.avg_squared_grad[key] + \\\n",
        "                                        (1 - self.rho) * np.square(grads[\"d\" + key])\n",
        "\n",
        "            # Compute RMS[g]\n",
        "            rms_grad = np.sqrt(self.avg_squared_grad[key] + self.epsilon)\n",
        "\n",
        "            # Compute RMS[Δθ]\n",
        "            rms_delta = np.sqrt(self.avg_squared_delta[key] + self.epsilon)\n",
        "\n",
        "            # Compute parameter update (Δθ)\n",
        "            delta = -self.learning_rate * (rms_delta / rms_grad) * grads[\"d\" + key]\n",
        "\n",
        "            # Update running average of squared parameter updates\n",
        "            self.avg_squared_delta[key] = self.rho * self.avg_squared_delta[key] + \\\n",
        "                                         (1 - self.rho) * np.square(delta)\n",
        "\n",
        "            # Update parameters\n",
        "            params[key] += delta\n",
        "\n",
        "        return params\n"
      ],
      "metadata": {
        "id": "8Za9BCrMfvas"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Nadam(Optimizer):\n",
        "    \"\"\"\n",
        "    Nesterov-accelerated Adaptive Moment Estimation\n",
        "\n",
        "    Formula (simplified):\n",
        "    m = β1 * m + (1 - β1) * ∇J(θ)               # First moment estimate\n",
        "    v = β2 * v + (1 - β2) * (∇J(θ))^2           # Second moment estimate\n",
        "    m̂ = m / (1 - β1^t)                          # Bias-corrected first moment\n",
        "    v̂ = v / (1 - β2^t)                          # Bias-corrected second moment\n",
        "    m̂_nesterov = (β1 * m̂) / (1 - β1^(t+1)) + ((1 - β1) * ∇J(θ)) / (1 - β1^t)  # Nesterov momentum\n",
        "    θ = θ - η * m̂_nesterov / (√v̂ + ε)           # Parameter update\n",
        "\n",
        "    Where:\n",
        "    m: first moment vector (mean of gradients)\n",
        "    v: second moment vector (uncentered variance)\n",
        "    β1, β2: decay rates for moment estimates\n",
        "    t: time step\n",
        "    η: learning rate\n",
        "    ε: small constant for numerical stability\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        super().__init__(learning_rate)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}  # First moment\n",
        "        self.v = {}  # Second moment\n",
        "        self.t = 0   # Time step\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if not self.m:\n",
        "            # Initialize moments if it's the first iteration\n",
        "            for key in params:\n",
        "                self.m[key] = np.zeros_like(params[key])\n",
        "                self.v[key] = np.zeros_like(params[key])\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        for key in params:\n",
        "            # Update biased first moment estimate\n",
        "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[\"d\" + key]\n",
        "            # Update biased second moment estimate\n",
        "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * np.square(grads[\"d\" + key])\n",
        "\n",
        "            # Compute bias-corrected first moment estimate\n",
        "            m_corrected = self.m[key] / (1 - self.beta1 ** self.t)\n",
        "            # Compute bias-corrected second moment estimate\n",
        "            v_corrected = self.v[key] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # Compute Nesterov accelerated momentum term\n",
        "            m_nesterov = (self.beta1 * m_corrected) / (1 - self.beta1 ** (self.t + 1)) + \\\n",
        "                         ((1 - self.beta1) * grads[\"d\" + key]) / (1 - self.beta1 ** self.t)\n",
        "\n",
        "            # Update parameters\n",
        "            params[key] -= self.learning_rate * m_nesterov / (np.sqrt(v_corrected) + self.epsilon)\n",
        "\n",
        "        return params"
      ],
      "metadata": {
        "id": "jZrF08b3fzZ7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Usage example with neural network\n",
        "\n",
        "def train_neural_network(model, optimizer, X, y, epochs=1000):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        output = model.forward(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = np.mean(np.square(y - output))\n",
        "\n",
        "        # Backward pass - calculate gradients\n",
        "        gradients = model.backward(X, y, output)\n",
        "\n",
        "        # Update parameters using optimizer\n",
        "        model.params = optimizer.update(model.params, gradients)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "# model = NeuralNetwork(...)\n",
        "# optimizer = Adam(learning_rate=0.001)\n",
        "# trained_model = train_neural_network(model, optimizer, X, y, epochs=5000)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "collapsed": true,
        "id": "FqdfKO0_f3IA",
        "outputId": "dd88013c-1345-4741-ccbb-9480f9ebeb2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUsage example with neural network\\n\\ndef train_neural_network(model, optimizer, X, y, epochs=1000):\\n    for epoch in range(epochs):\\n        # Forward pass\\n        output = model.forward(X)\\n        \\n        # Compute loss\\n        loss = np.mean(np.square(y - output))\\n        \\n        # Backward pass - calculate gradients\\n        gradients = model.backward(X, y, output)\\n        \\n        # Update parameters using optimizer\\n        model.params = optimizer.update(model.params, gradients)\\n        \\n        if epoch % 100 == 0:\\n            print(f\"Epoch {epoch}, Loss: {loss}\")\\n    \\n    return model\\n\\n# Example usage:\\n# model = NeuralNetwork(...)\\n# optimizer = Adam(learning_rate=0.001)\\n# trained_model = train_neural_network(model, optimizer, X, y, epochs=5000)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # When to Use Each Optimizer\n",
        "    \n",
        "## SGD (Stochastic Gradient Descent)\n",
        "- **When to use**: Simple problems with convex loss surfaces\n",
        "- **Advantages**: Simple to implement, well understood theoretically\n",
        "- **Disadvantages**: Slow convergence, gets stuck in local minima, sensitive to feature scaling\n",
        "- **Learning rate**: Typically 0.01 to 0.1\n",
        "\n",
        "## SGD with Momentum\n",
        "- **When to use**: When SGD is too slow or oscillates too much\n",
        "- **Advantages**: Faster convergence than SGD, can escape some local minima\n",
        "- **Disadvantages**: Needs momentum hyperparameter tuning, can overshoot minima\n",
        "- **Learning rate**: Typically 0.01 to 0.1, with momentum 0.9 to 0.99\n",
        "\n",
        "## RMSprop\n",
        "- **When to use**: Non-stationary objectives, problems with sparse gradients\n",
        "- **Advantages**: Adapts learning rate per parameter, good for RNNs and computer vision\n",
        "- **Disadvantages**: Requires more computation than SGD\n",
        "- **Learning rate**: Typically 0.001 to 0.01\n",
        "\n",
        "## Adam\n",
        "- **When to use**: Deep learning models, noisy gradients, large datasets\n",
        "- **Advantages**: Combines benefits of momentum and RMSprop, robust to hyperparameters\n",
        "- **Disadvantages**: Can converge to sub-optimal solutions in some cases\n",
        "- **Learning rate**: Typically 0.001 to 0.0001\n",
        "\n",
        "## AdaGrad\n",
        "- **When to use**: Sparse data, NLP problems\n",
        "- **Advantages**: Good for sparse features, different learning rates per parameter\n",
        "- **Disadvantages**: Learning rate decreases over time, may stop learning too early\n",
        "- **Learning rate**: Typically 0.01 to 0.1\n",
        "\n",
        "## Adadelta\n",
        "- **When to use**: When you want AdaGrad's benefits without decreasing learning rates\n",
        "- **Advantages**: No need to set learning rate manually, robust to large gradients\n",
        "- **Disadvantages**: More computationally intensive\n",
        "- **Learning rate**: Not critical, often set to 1.0\n",
        "\n",
        "## Nadam\n",
        "- **When to use**: When you want the benefits of both NAG and Adam\n",
        "- **Advantages**: Faster convergence than Adam in many cases\n",
        "- **Disadvantages**: Slightly more computation than Adam\n",
        "- **Learning rate**: Typically 0.002 to 0.0002\n",
        "\n",
        "## General Recommendations\n",
        "\n",
        "- **Start with Adam**: It's a good default optimizer for most problems\n",
        "- **Use SGD with momentum**: For fine-tuning or if you suspect Adam is converging to poor solutions\n",
        "- **RMSprop**: Good alternative to Adam, especially for RNNs\n",
        "- **For sparse data**: Try AdaGrad or sparse variants of Adam\n",
        "- **If all else fails**: Grid search over optimizers and their hyperparameters\n",
        "\n",
        "## Special Cases\n",
        "\n",
        "- **Computer Vision**: Adam, RMSprop, or SGD with momentum (with learning rate scheduling)\n",
        "- **NLP**: Adam or variants like AdamW (Adam with weight decay)\n",
        "- **Reinforcement Learning**: Often RMSprop or Adam\n",
        "- **Generative Models (GANs)**: Adam for generator, SGD for discriminator often works well\n"
      ],
      "metadata": {
        "id": "zOVtsGiChMm-"
      }
    }
  ]
}