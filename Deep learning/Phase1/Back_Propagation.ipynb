{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Algorithm: Mathematical Explanation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Backpropagation is a supervised learning algorithm for training artificial neural networks. It works by calculating the gradient of the loss function with respect to the network's weights, allowing the weights to be updated in the direction that minimizes the loss. This document provides a mathematical explanation of the backpropagation algorithm.\n",
        "\n",
        "## Notation\n",
        "\n",
        "- $x_i$: Input features\n",
        "- $y$: True output (target)\n",
        "- $\\hat{y}$: Predicted output\n",
        "- $W^{[l]}$: Weight matrix for layer $l$\n",
        "- $b^{[l]}$: Bias vector for layer $l$\n",
        "- $z^{[l]}$: Weighted input for layer $l$\n",
        "- $a^{[l]}$: Activation output for layer $l$\n",
        "- $\\sigma()$: Activation function (e.g., sigmoid)\n",
        "- $L$: Loss function (e.g., Mean Squared Error)\n",
        "- $\\delta^{[l]}$: Error term for layer $l$\n",
        "- $\\eta$: Learning rate\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "The forward propagation process computes the network's output given an input:\n",
        "\n",
        "1. **Input Layer**: The input layer simply passes the input features\n",
        "   $$a^{[0]} = x$$\n",
        "\n",
        "2. **Hidden Layers**: For each hidden layer $l$ (where $l = 1, 2, ..., L-1$):\n",
        "   $$z^{[l]} = a^{[l-1]} W^{[l]} + b^{[l]}$$\n",
        "   $$a^{[l]} = \\sigma(z^{[l]})$$\n",
        "\n",
        "3. **Output Layer**: The final layer produces the prediction:\n",
        "   $$z^{[L]} = a^{[L-1]} W^{[L]} + b^{[L]}$$\n",
        "   $$\\hat{y} = a^{[L]} = \\sigma(z^{[L]})$$\n",
        "\n",
        "## Loss Calculation\n",
        "\n",
        "After obtaining the prediction, we calculate the loss to measure how far the prediction is from the true value. For a mean squared error loss:\n",
        "\n",
        "$$L = \\frac{1}{2} (\\hat{y} - y)^2$$\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "The core of backpropagation is computing the gradients of the loss with respect to each parameter. This is done by applying the chain rule of calculus repeatedly:\n",
        "\n",
        "1. **Output Layer Error**: First, calculate the error at the output layer:\n",
        "   $$\\delta^{[L]} = (\\hat{y} - y) \\odot \\sigma'(z^{[L]})$$\n",
        "\n",
        "   For the MSE loss with sigmoid activation, this simplifies to:\n",
        "   $$\\delta^{[L]} = (\\hat{y} - y) \\odot \\hat{y} \\odot (1 - \\hat{y})$$\n",
        "\n",
        "   If using cross-entropy loss with softmax, this further simplifies to:\n",
        "   $$\\delta^{[L]} = \\hat{y} - y$$\n",
        "\n",
        "2. **Backpropagate Error**: For each previous layer $l = L-1, L-2, ..., 1$:\n",
        "   $$\\delta^{[l]} = (\\delta^{[l+1]} W^{[l+1]T}) \\odot \\sigma'(z^{[l]})$$\n",
        "\n",
        "   Where $\\sigma'(z^{[l]})$ is the derivative of the activation function, and $\\odot$ represents element-wise multiplication.\n",
        "\n",
        "3. **Compute Gradients**: Calculate the gradients for weights and biases:\n",
        "   $$\\frac{\\partial L}{\\partial W^{[l]}} = a^{[l-1]T} \\delta^{[l]}$$\n",
        "   $$\\frac{\\partial L}{\\partial b^{[l]}} = \\sum \\delta^{[l]}$$\n",
        "\n",
        "## Weight Update\n",
        "\n",
        "Finally, update the weights and biases using gradient descent:\n",
        "\n",
        "$$W^{[l]} = W^{[l]} - \\eta \\frac{\\partial L}{\\partial W^{[l]}}$$\n",
        "$$b^{[l]} = b^{[l]} - \\eta \\frac{\\partial L}{\\partial b^{[l]}}$$\n",
        "\n",
        "## Derivation of the Backpropagation Equations\n",
        "\n",
        "### Why Backpropagation Works: The Chain Rule\n",
        "\n",
        "Backpropagation is based on the chain rule from calculus. For a composite function $f(g(x))$, the chain rule states:\n",
        "\n",
        "$$\\frac{d}{dx}f(g(x)) = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
        "\n",
        "In a neural network, we're trying to find $\\frac{\\partial L}{\\partial W^{[l]}}$, which requires multiple applications of the chain rule.\n",
        "\n",
        "### Deriving the Output Layer Error\n",
        "\n",
        "For the output layer, we want to compute $\\frac{\\partial L}{\\partial W^{[L]}}$.\n",
        "\n",
        "Using the chain rule:\n",
        "$$\\frac{\\partial L}{\\partial W^{[L]}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{[L]}} \\cdot \\frac{\\partial z^{[L]}}{\\partial W^{[L]}}$$\n",
        "\n",
        "Breaking down each term:\n",
        "1. $\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y$ (for MSE loss)\n",
        "2. $\\frac{\\partial \\hat{y}}{\\partial z^{[L]}} = \\sigma'(z^{[L]}) = \\hat{y}(1-\\hat{y})$ (for sigmoid activation)\n",
        "3. $\\frac{\\partial z^{[L]}}{\\partial W^{[L]}} = a^{[L-1]}$\n",
        "\n",
        "Combining these:\n",
        "$$\\frac{\\partial L}{\\partial W^{[L]}} = a^{[L-1]T} \\cdot [(\\hat{y} - y) \\odot \\hat{y} \\odot (1 - \\hat{y})]$$\n",
        "\n",
        "The term in brackets is our $\\delta^{[L]}$.\n",
        "\n",
        "### Deriving the Hidden Layer Error\n",
        "\n",
        "For a hidden layer $l$, we need to compute $\\frac{\\partial L}{\\partial W^{[l]}}$.\n",
        "\n",
        "Using the chain rule:\n",
        "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \\frac{\\partial z^{[l]}}{\\partial W^{[l]}}$$\n",
        "\n",
        "The challenge is computing $\\frac{\\partial L}{\\partial a^{[l]}}$, which requires backpropagating the error from the next layer.\n",
        "\n",
        "For $\\frac{\\partial L}{\\partial a^{[l]}}$, we can write:\n",
        "$$\\frac{\\partial L}{\\partial a^{[l]}} = \\frac{\\partial L}{\\partial z^{[l+1]}} \\cdot \\frac{\\partial z^{[l+1]}}{\\partial a^{[l]}} = \\delta^{[l+1]} \\cdot W^{[l+1]}$$\n",
        "\n",
        "Therefore:\n",
        "$$\\delta^{[l]} = (\\delta^{[l+1]} W^{[l+1]T}) \\odot \\sigma'(z^{[l]})$$\n",
        "\n",
        "## Practical Considerations\n",
        "\n",
        "1. **Vanishing/Exploding Gradients**: The repeated multiplication in backpropagation can cause gradients to become very small (vanishing) or very large (exploding). This is why activation functions like ReLU are often preferred over sigmoid for deep networks.\n",
        "\n",
        "2. **Batch vs. Stochastic Gradient Descent**: In practice, gradients are often computed over batches of examples rather than single examples to reduce noise and improve convergence.\n",
        "\n",
        "3. **Learning Rate Scheduling**: Decreasing the learning rate over time can help the algorithm converge to a better minimum.\n",
        "\n",
        "4. **Regularization**: Adding regularization terms to the loss function can help prevent overfitting by penalizing large weights.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Backpropagation is an efficient algorithm that leverages the chain rule of calculus to compute gradients in a neural network. By propagating the error backwards through the network, it allows for the efficient training of deep neural networks with many layers."
      ],
      "metadata": {
        "id": "1gJzu5l2bjbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5zdmUIhafyC",
        "outputId": "49a49431-6c3a-49eb-e68e-ad2f1b619c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2876340734600314\n",
            "Epoch 1000, Loss: 0.00016447049721096598\n",
            "Epoch 2000, Loss: 2.2384874487956974e-05\n",
            "Epoch 3000, Loss: 7.914683336980375e-06\n",
            "Epoch 4000, Loss: 3.911966390881609e-06\n",
            "Epoch 5000, Loss: 2.2968352803879744e-06\n",
            "Epoch 6000, Loss: 1.4977122945545404e-06\n",
            "Epoch 7000, Loss: 1.048069879194837e-06\n",
            "Epoch 8000, Loss: 7.715908490859064e-07\n",
            "Epoch 9000, Loss: 5.901688020089188e-07\n",
            "\n",
            "Predictions after training:\n",
            "Input: [0 0], Target: [0], Prediction: 0.0005\n",
            "Input: [0 1], Target: [1], Prediction: 0.9994\n",
            "Input: [1 0], Target: [1], Prediction: 0.9992\n",
            "Input: [1 1], Target: [0], Prediction: 0.0008\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simple Neural Network with Backpropagation Implementation\n",
        "# This example shows a 2-layer neural network (input → hidden → output)\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"Initialize the neural network with random weights\"\"\"\n",
        "        # Initialize weights with larger random values to break symmetry\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        # Clip values to avoid numerical instability\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of sigmoid function\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # First layer computation\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)  # Hidden layer activation\n",
        "\n",
        "        # Second layer computation\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)  # Output layer activation\n",
        "\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        \"\"\"Backward pass: calculating gradients with backpropagation\"\"\"\n",
        "        # Number of samples\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Calculate error at the output layer (MSE derivative)\n",
        "        # Note: For MSE loss, the error is (output - y), not (output - y)\n",
        "        self.dz2 = output - y\n",
        "\n",
        "        # Backpropagate error to hidden layer weights\n",
        "        self.dW2 = (1/m) * np.dot(self.a1.T, self.dz2)\n",
        "        self.db2 = (1/m) * np.sum(self.dz2, axis=0, keepdims=True)\n",
        "\n",
        "        # Backpropagate error to hidden layer\n",
        "        self.dz1 = np.dot(self.dz2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for input to hidden layer weights\n",
        "        self.dW1 = (1/m) * np.dot(X.T, self.dz1)\n",
        "        self.db1 = (1/m) * np.sum(self.dz1, axis=0, keepdims=True)\n",
        "\n",
        "    def update_parameters(self, learning_rate):\n",
        "        \"\"\"Update weights using calculated gradients\"\"\"\n",
        "        # Update weights with gradient descent\n",
        "        self.W1 -= learning_rate * self.dW1\n",
        "        self.b1 -= learning_rate * self.db1\n",
        "        self.W2 -= learning_rate * self.dW2\n",
        "        self.b2 -= learning_rate * self.db2\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        \"\"\"Train the neural network\"\"\"\n",
        "        losses = []\n",
        "        for i in range(epochs):\n",
        "            # Forward propagation\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Compute loss (Mean Squared Error)\n",
        "            loss = np.mean(np.square(y - output))\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Backward propagation\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "            # Update parameters\n",
        "            self.update_parameters(learning_rate)\n",
        "\n",
        "            # Print loss every 1000 epochs\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Epoch {i}, Loss: {loss}\")\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a simple XOR problem\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    # Initialize neural network with 2 input neurons, 8 hidden neurons, 1 output neuron\n",
        "    # XOR needs more hidden neurons to learn the non-linear separation\n",
        "    nn = SimpleNeuralNetwork(2, 8, 1)\n",
        "\n",
        "    # Train network with higher learning rate and more epochs\n",
        "    nn.train(X, y, learning_rate=1.0, epochs=10000)\n",
        "\n",
        "    # Test network\n",
        "    predictions = nn.forward(X)\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    for i in range(len(X)):\n",
        "        print(f\"Input: {X[i]}, Target: {y[i]}, Prediction: {predictions[i][0]:.4f}\")"
      ]
    }
  ]
}